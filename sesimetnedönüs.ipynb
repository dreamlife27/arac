{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1pLWN3bu0GjxENbPyJMMi3vi4jJB5BU1D",
      "authorship_tag": "ABX9TyPErd35bvXpy2ubhM3MKqzc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dreamlife27/arac/blob/main/sesimetned%C3%B6n%C3%BCs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper'ı yükleme\n",
        "!pip install -q openai-whisper ffmpeg\n",
        "!sudo apt update && sudo apt install -y ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-0YI6uBX5sz7",
        "outputId": "166d0355-110a-4511-e860-da4616b31c8c",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/800.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/800.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m798.7/800.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Get:1 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [3,972 kB]\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [2,773 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,241 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:13 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [8,793 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,540 kB]\n",
            "Get:16 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,686 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [3,081 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [4,126 kB]\n",
            "Fetched 28.6 MB in 3s (11.4 MB/s)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "39 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "\u001b[1;33mW: \u001b[0mSkipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\u001b[0m\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 39 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import torch\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "6QXaP4Py6MJu"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU kontrolü ve T4 GPU doğrulama\n",
        "use_gpu = torch.cuda.is_available()\n",
        "gpu_name = torch.cuda.get_device_name(0) if use_gpu else \"GPU bulunamadı\"\n",
        "print(f\"Kullanılan GPU: {gpu_name}\")\n",
        "\n",
        "if use_gpu and \"T4\" not in gpu_name:\n",
        "    print(\"Uyarı: T4 GPU kullanmıyorsunuz. Performans farklı olabilir!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SnW8LDHx6RkL",
        "outputId": "1e35dfa7-9648-4d81-ccba-88f5b49c29c0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Kullanılan GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Whisper modelini yükleme (GPU varsa kullan, yoksa CPU ile çalıştır)\n",
        "device = \"cuda\" if use_gpu else \"cpu\"\n",
        "model = whisper.load_model(\"\").to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8v0rPUR6WTP",
        "outputId": "69d2d2fb-3fe9-4d69-d015-1788db013272"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:32<00:00, 46.7MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kullanıcıdan dosya yüklemesini isteme\n",
        "print(\"Lütfen bir ses dosyası yükleyin (MP3, WAV vb.)\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Yüklenen dosyanın adını alma\n",
        "filename = list(uploaded.keys())[0]\n",
        "\n",
        "# Dosyayı metne çevirme (Türkçe dili belirterek, cümleleri mantıklı hale getirme)\n",
        "print(\"Transkripsiyon yapılıyor, lütfen bekleyin...\")\n",
        "result = model.transcribe(filename, language=\"en\", word_timestamps=False)\n",
        "\n",
        "# Sonucu ekrana yazdırma\n",
        "print(\"\\nDönüştürülen Metin:\")\n",
        "print(result[\"text\"])\n",
        "\n",
        "# Metni bir dosyaya kaydetme\n",
        "with open(\"transcription.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(result[\"text\"])\n",
        "\n",
        "print(\"\\nMetin 'transcription.txt' dosyasına kaydedildi.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "JE7-WbSR6tG0",
        "outputId": "9ffc973e-1e2a-4ffb-efa0-f4a937029e61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lütfen bir ses dosyası yükleyin (MP3, WAV vb.)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-658f041c-3562-4349-b3c2-9998e44befcf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-658f041c-3562-4349-b3c2-9998e44befcf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Whitepaper Companion Podcast - Foundational LLMs & Text Generation [Na3O4Pkbp-U].mp3 to Whitepaper Companion Podcast - Foundational LLMs & Text Generation [Na3O4Pkbp-U] (2).mp3\n",
            "Transkripsiyon yapılıyor, lütfen bekleyin...\n",
            "\n",
            "Dönüştürülen Metin:\n",
            " All right, welcome everyone to the deep dive. Today we're taking a deep dive into something pretty huge, foundational large language models or LLMs and how they create text. I mean, it seems like they're popping up everywhere, right? Changing how we write code, how we even write stories. Yeah, the advancements have been incredibly fast. It's hard to keep up. For this deep dive, we're going all the way up to February 2025. So we're talking cutting edge stuff. Yeah, seriously cutting edge. So our mission today is to distill all that down, get to the core of these LLMs. What are they made of? How do they evolve? How do they actually learn? Of course, how do we even measure how good they are? We're going to look at all that. And even some of the tricks used to make them run faster. It's a lot to cover, but hopefully we can make it a fun ride. The starting point for all this, the foundation of most modern LLMs, is the transformer architecture. And it's actually kind of funny. It came from a Google project focused on language translation back in 2017. OK, so this transformer thing I remember hearing about that. The original one had this encoder and decoder, right? Like it would take a sentence in one language and turn it into another language. Yeah, exactly. So the encoder would take the input, you know, like a sentence in French, and create this representation of it, kind of like a summary of the meaning. Then the decoder uses that representation to generate the output, like the English translation, piece by piece. And each piece, they call it a token. It could be a whole word like cat or a part of word and prefix. But the real magic is what happens inside each layer of this transformer thing. All right, well, let's get into that magic. What's actually going on in the transformer layer? So first things first. The input text needs to be prepped for the model, right? We turn the text into those tokens based on a specific vocabulary the model uses. And each of these tokens gets turned into this dense vector, we call it, in embedding, that captures the meaning of that token. But, and this is important, transformers process all the tokens at the same time. So we need to add in some information about the order they appeared in the sentence that's called positional encoding. And there are different types of positional encoding, like sinusoidal and learned encodings. The choice can actually subtly affect how well the model understands longer sentences or longer sequences of text. Makes sense. Otherwise, it's like just throwing all the words in a bag, you lose all the structure. Then we get to, I think, the most famous part, the multi-head attention. I saw this thirsty tiger example. I thought it was pretty helpful to try and understand self-attention. Oh, yeah. The thirsty tiger, a classic. So the sentence is, the tiger jumped out of a tree to get a drink because it was thirsty. Now, self-attention, it's what lets the model figure out that it refers back to the tiger. And it does this by creating these vectors, query, key, and value vectors for every single word. Okay, so wait, let me try this. So that would be the query. It's like asking, hey, which other words in this sentence are important to understanding me? Yeah, you got it. And the key, it's like a label attached to each word telling you what it represents. Then the value, that's the actual information the word carries. So like it looks at all the other words' keys and sees that the tiger has a key that's really similar, so it pays more attention to the tiger. Exactly. And the model calculates the score for how well each query matches up with all the other keys. Then it normalizes these scores so they become weights, attention weights. These weights tell you how much each word should pay attention to the others. Then it uses those weights to create a weighted sum of all the value vectors. And what you get is this rich representation for each word, which takes into account its relationship to every other word in the sentence. And the really cool part is all of this, all this comparison and calculation happens in parallel using these matrices for the query, Q, key, K, and value V of all the tokens. This ability to process all these relationships at the same time is a huge reason why transformers are so good at capturing these subtle meanings in language that previous models, the sequential ones, really struggled with, especially across longer distances within a sentence. Okay, I think I'm starting to get it. And multi-edges means doing the self-attention thing like several times at the same time, right? But with different sets of those query, key, and value matrices. Yes. And each head, each of these parallel self-attention processes learns to focus on different types of relationships. One head might look for grammatical stuff. Another one might focus on the meaning connections between words. And by combining all those different views, those different perspective, the model gets this much deeper understanding of what's going on in the text. It's like getting a second opinion or a third or a fourth. It's powerful stuff. Now, I also saw these terms, layer normalization and residual connections. They seem to be important for keeping the training on track, especially when you have these really deep networks. They're essential. Layer normalization, it helps to keep the activity level of each layer, the activations, at a steady level. That makes the training go much faster and usually gives you better results in the end. Residual connections, they act like shortcuts within the network. It's like they let the original input of a layer bypass everything and get added directly to the output. So it's a way for the network to remember what it learned earlier, even if it's gone through many, many layers. Exactly. That's why they're so important in these really deep models. It prevents that vanishing radiance problem where the signal gets weaker and weaker as it goes deeper. Then after all that, we have the feedforward layer. Right, the feedforward layer. Yeah, it's this network, a feedforward network that's applied to each token's representation separately after we've done all that attention stuff. It usually has two linear transformations with what's called a nonlinear activation function in between, like re-lu or g-e-l-u. This gives the model even more power to represent information, helps it learn these complex functions of the input. So we've talked about encoders and decoders in the original transformer design, but I noticed in the materials that many of the newer LLMs, they're going with a decoder-only architecture. What's the advantage of just using the decoder? Well, you see, when you're focused on generating texts, like writing or having a conversation, you don't always need the encoder part. The encoder's main job is to create this representation of the whole input sequence upfront. Decoder-only models, they kind of skip that step and directly generate the output token by token. They use this special type of self-attention called masked self-attention. It's a way to make sure that when the model is predicting the next token, it can only see the tokens that came before it, just like when we write or speak. So it's a simpler design, and it makes sense for generating text. Exactly. And before we move on from architecture, there's one more thing. Mixture of Experts, or MoE, it's this really clever way to make these models even bigger but without making them super slow. I was just gonna ask about that. How do you make these massive models more efficient? MoE seems to be a key part of that. It really is. So in MoE, you have these specialized sub-models, these experts, right? And they all live within one big model. But the trick is, there's this gating network that decides which experts are the best ones to use for each input. So you might have a model with billions of parameters, but with any given input, only a small fraction of those parameters, those experts, are actually active. It's like having a team of specialists, and you only call in the ones you need for the specific job. Makes sense. Yeah, it's all about efficiency. Now, I think it would be good to step back and look at the big picture, how LLMs have evolved over time. You know, the transformer was the spark, but then things really started taking off. Yeah, there's this whole family tree of LLMs now. Where did it all begin after that first transformer paper? Well, GPT-1 from OpenAI in 2018 was a real turning point. It was decoder only, and they trained it in an unsupervised way on this massive data set of books. They called it Bookscorpus. This unsupervised pre-training was key. It let the model learn general language patterns from all this raw text. Then they would fine tune it for specific tasks. But GPT-1 had its limitations, right? I remember reading that sometimes it would get stuck, repeating the same phrases over and over. Yeah, it wasn't perfect. Sometimes the text would get a bit repetitive, and it wasn't so good at long conversations, but it was still a major step. Then, that same year, Google came out with BERT. Now, BERT was different. It was encoder only, and its focus was on understanding language, not generating it. It was trained on these tasks, like masked language modeling and next-sense prediction, which are all about figuring out the meaning of text. So GPT-1 could talk, but sometimes it would get stuck, and BERT could understand, but couldn't really hold a conversation. That's a good way to put it. Then came GPT-2 in 2019, also from OpenAI. They took the GPT-1 idea and just scaled it up way more data from this dataset called WebText, which was taken from Reddit, and many more parameters in the model itself. The result, much better coherence. It could handle longer dependencies between words. And the really cool thing was it could learn new tasks without even being specifically trained on them. They call it zero-shot learning. You just show it an example of the task in the prompt, and it could often figure out how to do it. Whoa, just from an example, that's amazing. It was quite a leap. And then starting in 2020, we got the GPT-3 family. These models just kept getting bigger and bigger, billions of parameters. GPT-3, with its 175 billion parameters, it was huge. And it got even better at few-shot learning, learning from just a handful of examples. We also saw these instruction-tuned models like InstructGPT, trained specifically to follow instructions written in natural language. Then came models like GPT-3.5, which were amazing at understanding and writing code. And GPT-4, that was a game changer, a truly multimodal model. It could handle images and text together. The context window size also exploded, meaning it could consider much longer pieces of text at once. And Google, they were pushing things forward as well, right? I remember LambDA, their conversational AI, was a big deal. Absolutely. LambDA came out in 2021, and it was designed from the ground up for natural sounding conversations. While the GPTs were becoming more general purpose, LambDA was all about dialogue, and it really showed. Then DeepMind got in on the action with Gopher in 2021. Gopher, what made that one stand out? Gopher was another big decoder-only model. But DeepMind, they really focused on using high-quality data for training, a data set they called Massive Text. And they also used some pretty advanced optimization techniques. Gopher did really well on knowledge-intensive tasks, but it still struggled with more complex reasoning problems. One interesting thing they found was that just making the model bigger, adding more parameters, doesn't help with every type of task. Some tasks need different approaches. Right, it's not just about size. Then there was Glam from Google, which used this mixture of experts idea we were talking about earlier, making those huge models run much faster. Exactly. Gram showed that you could get the same or even better performance than a dense model like GPT-3, but use way less compute power. It was a big step forward in efficiency. Then came Chinchilla in 2022, also from DeepMind. They really challenged those scaling laws, the idea that bigger is always better. Yeah, Chinchilla was a really important paper. They found that for a given number of parameters, you should actually train on a much larger data set than people were doing before. They had this 70 billion parameter model that actually outperformed much larger models because they trained it on this huge amount of data. It really changed how people thought about scaling. It's not just about the size of the model. It's also about the size of the data you trained it on. Yeah, exactly. And then Google released PaulM and PaulM2. PaulM came out in 2022 and had really impressive performance on all kinds of benchmarks. Part of that was because of Google's Pathway system, which made it easier to scale up models efficiently. PaulM2 came out in 2023, and it was even better at things like reasoning, coding, and math, even though it actually had fewer parameters than the first PaulM. PaulM2 is now the foundation for a lot of Google's generative AI stuff in Google Cloud. And then we have Gemini, Google's newest family of models, which are multimodal right from the start. Yeah, Gemini is really pushing the boundaries. It's designed to handle not just text, but also images, audio, and video. They've been working on architectural improvements that let them scale these models up really big, and they've optimized Gemini to run really fast on their tensor processing units, TPUs. They also use MoE in some of the Gemini models. There are different sizes too, Ultra, Pro, Nano, and Flash, each for different needs. Gemini 1.5 Pro, with its massive context window, that's been particularly impressive. It can handle millions of tokens, which is incredible. It's mind boggling how fast these context windows are growing. What about the open source side of things? There's a lot happening there too, right? Oh, absolutely. The open source LLM community is exploding. Google released Gemma and Gemma 2 in 2024, which are these lightweight but very powerful open models building off of their Gemini research. Gemma has a huge vocabulary, and there's even a 2 billion parameter version that can run on a single GPU, so it's much more accessible. Gemma 2 is performing comparably to much bigger models, like Meta's Llama 370B. Meta's Llama family has been really influential, starting with Llama 1, then Llama 2, which had a commercial use license, and now Llama 3. They've been improving in areas like reasoning, coding, general knowledge, safety, and they've even added multilingual and vision models in the Llama 3.2 release. Mistral AI, they have Mixtral, which uses a sparse mixture of experts set up, eight experts, but only two are active at any given time. It's great at math, coding, and multilingual tasks, and many of their models are open source. Then you have OpenAI's O1 models, which are all about complex reasoning. They're getting top results in these really challenging scientific reasoning benchmarks. DeepSeq has also been doing some really interesting work on reasoning using this new reinforcement learning technique called group relative policy optimization. Their DeepSeq R1 model is comparable to OpenAI's O1 on many tasks, although it's still closed source, even though they released the model weights. And beyond those, there are tons of other open models being developed all the time, like Q1.5 from Alibaba, Yi from OO1 AI, and Grok3 from XAI. It's a really exciting space, but it's important to check the licenses on those open models before you use them. Yeah, keeping up with all these models is a full-time job in itself. It's incredible. It is. And you know, all these models, all these advancements, they're all built on that basic transformer architecture we talked about earlier. Right. But these foundational models, they're powerful. But they need to be tailored for specific tasks. And that's where fine-tuning comes in. Exactly. So training in LLM usually involves two main steps. First, you have pre-training. You feed the model tons and tons of data, just raw text, no labels. This lets it learn the basic patterns of language, how words and sentences work together. It's like learning the grammar and vocabulary of a language. Pre-training is super resource intensive. It takes huge amounts of compute power. It's like giving the model a general education in language. Exactly. Then comes fine-tuning. You take that pre-trained model, which has all that general knowledge, and you train it further on a smaller, more targeted data set. This data set is specific to the task you want it to do, like translating languages, writing different kinds of creative text formats, or answering questions. So you're specializing the model, making it an expert in a particular area. And supervised fine-tuning, or SFT, that's one of the main techniques used for this, right? Yeah. SFT is really common. It involves training the model on labeled examples, where you have a prompt and the desired response. So for example, if you want it to answer questions, you get lots of examples of questions and the correct answers. This helps the model learn how to perform that specific task, and also helps to shape its overall behavior. So you're not just teaching it what to do. You're also teaching it how to behave. Exactly. You want it to be helpful, safe, and good at following instructions. And then there's reinforcement learning from human feedback, or RLHF. This is a way to make the model's output more aligned with what humans actually prefer. I was wondering about that. How do you teach these models to be more human-like in their responses? Well, RLHF is a big part of that. It's not just about giving the model correct answers. It's about teaching it to generate responses that humans find helpful, truthful, and safe. They do this by training a separate reward model based on human preferences. So you might have human evaluators rank different responses from the LLM, telling you which ones they like better. Then this reward model is used to fine-tune the LLM using reinforcement learning algorithms. So the LLM learns to generate responses that get higher rewards from the reward model, which is based on what humans prefer. There are also some newer techniques, like reinforcement learning from AI feedback, RL-AIF, and direct preference optimization, DPO, that are trying to make this alignment process even better. It's fascinating how much human input goes into making these models more human-like. Now, fully fine-tuning these massive models, it sounds computationally expensive. Are there ways to adapt them to new tasks without having to retrain the whole thing? Yeah, that's a good point. Fully fine-tuning these huge models, it can be really expensive. So people have developed these techniques called parameter-efficient fine-tuning, or PEFT. The idea is to only train a small part of the model, leaving most of the pre-trained weights frozen. This makes fine-tuning much faster and cheaper. So it's like just making small adjustments instead of overhauling the entire system. What are some examples of these PEFT techniques? One popular method is adapter-based fine-tuning. You add these small modules called adapters into the model, and you only train the parameters within those adapters. The original weights stay the same. Another one is low-rank adaptation, or LoRa. In LoRa, you use low-rank matrices to approximate the changes you would make to the original weights during full fine-tuning. This drastically reduces the number of parameters you need to train. There's also QLoRa, which is like LoRa, but even more efficient because it uses quantized weights. And then there's soft-prompting, where you learn the small vector, a soft prompt, that you add to the input. This soft prompt helps the model perform the desired task without changing the original weights. So it sounds like there are several different approaches to fine-tuning, and each one has its own trade-offs between performance, cost, and efficiency. Exactly. And these PEFT techniques are making it possible for more people to use and customize these powerful LLMs. It's really democratizing the technology. Now, once you have a fine-tuned model, how do you actually use it effectively? Prompt engineering seems to be a key skill here. Oh, it's absolutely essential. Prompt engineering is all about designing the input you give to the model, the prompt, in a way that gets you the output you're looking for. It can make a huge difference in the quality and relevance of the model's response. So what are some good prompt engineering techniques? There are a few that are really commonly used. Zero-shot prompting is where you give the model a direct instruction or question without giving it any examples. You're relying on its preexisting knowledge. Few-shot prompting is similar, but you give it a few examples to help it understand the format and style you're looking for. And for more complex reasoning tasks, chain of thought prompting is really useful. You basically show the model how to think through the problem step by step, which often leads to better results. It's like teaching it how to break down a complex problem into smaller, more manageable steps. Exactly. And then there's the way the model actually generates text, the sampling techniques. These can have a big impact on the quality, creativity, and diversity of the output. Yeah, I was curious about that. What are some of the different sampling techniques? Well, the simplest is greedy search, where the model always picks the most likely next token. This is fast, but can lead to repetitive output. Random sampling, as the name suggests, introduces more randomness, which can lead to more creative outputs, but also a higher chance of getting nonsensical text. Temperature is a parameter you can adjust to control this randomness. Higher temperature, more randomness. Top-K sampling limits the model's choices to the top-K most likely pokens, which helps to control the output. Top-P sampling, also called nucleus sampling, is similar, but uses a dynamic threshold based on the probabilities of the tokens. And finally, best-of-end sampling generates multiple responses and then picks the best one based on some criteria. So fine-tuning these sampling parameters is key to getting the kind of output you want, whether it's factual and accurate or more creative and imaginative. Yeah, it's a powerful tool. Now, I think it's time we talk about how we actually know if these models are any good. How do we evaluate their performance? That's a great question. Evaluating these LLMs, it's not like traditional machine learning tasks where you have a clear right or wrong answer. How do you measure something as subjective as the quality of generated text? It's definitely challenging, especially as we're trying to move beyond those early demos to real-world applications. Those traditional metrics like accuracy or F1 score, they don't really capture the whole picture when you're dealing with something as open-ended as text generation. So what does a good evaluation framework look like for LLMs? It needs to be multifaceted, that's for sure. First, you need data specifically designed for the task you're evaluating. This data should reflect what the model will see in the real world and should include real user interactions as well as synthetic data to cover all kinds of situations. Second, you can't just evaluate the model in isolation. You need to consider the whole system it's part of, like if you're using retrieval, augmented generation, REG, or if the LLM is controlling an agent. And lastly, you need to define what good actually means for your specific use case. It might be about accuracy, but it might also be about things like helpfulness, creativity, factual correctness, or adherence to a certain style. It sounds like you need to tailor your evaluation to the specific application. What are some of the main methods used for evaluating LLMs? We still use traditional quantitative methods, comparing the model's output to some ground truth answers, using metrics like BLEU or Rouge. But these metrics don't always capture the nuances of language. Sometimes a creative or unexpected response might be just as good or even better than the expected one. That's why human evaluation is so important. Human reviewers can provide more nuanced judgments on things like fluency, coherence, and overall quality. But of course, human evaluation is expensive and time consuming. So people have started using LLM-powered moderators. So you're using AI to judge other AI. Exactly. It sounds strange, but it can be quite effective. You basically give the moderator model the task, the evaluation criteria, and the responses generated by the model you're testing. The moderator then gives you a score, often with a reason for its judgment. There are different types of moderators, too. Generative models, reward models, and discriminative models. But one important thing is that you need to calibrate these moderators, meaning you need to compare their judgments to human judgments to make sure they're actually measuring what you want them to measure. You also need to be aware of the limitations of the moderator model itself. And there are even more advanced approaches being developed, like breaking down tasks into subtasks and using rubrics with multiple criteria to make the evaluation more interpretable. This is especially useful for evaluating multimodal generation, where you might need to assess the quality of the text, images, or videos separately. It sounds as evaluation is a complex area, but really important for making sure these models are reliable and actually useful in the real world. Now, all these models, they can be incredibly large. And getting responses from them can take time. What are some ways to speed up the inference process, make them respond faster? Yeah, as these models get bigger, they also get slower and more expensive to run. So optimizing inference, the process of generating responses is really important, especially for applications where speed is critical. So what are some of the techniques used to accelerate inference? Well, there are different approaches, but a lot of it comes down to trade-offs. You often have to balance the quality of the output with the speed and cost of generating it. So sometimes you might sacrifice little accuracy to gain a lot of speed? Exactly. And you also need to consider the trade-off between the latency of a single request, how long it takes to get one response, and the overall throughput of the system, how many requests it can handle per sector. The best approach depends on the application. Now, we can broadly categorize these techniques into two groups. There are the output approximating methods, which might involve changing the output slightly to gain efficiency. And then there are the output preserving methods, which keep the output exactly the same but try to optimize the computation. Let's start with the output approximating methods. I know quantization is a popular technique. Yeah, quantization is all about reducing the numerical precision of the model's weights and activations. So instead of using 32-bit floating point numbers, you might use 8-bit or even 4-bit integers. This saves a lot of memory and makes the calculations faster, often with only a very small drop in accuracy. There are also techniques like quantization aware training, QAT, which can help to minimize those accuracy losses. And you can even fine tune the quantization strategy itself. What about distillation? Isn't that where you train a smaller model to mimic a larger one? Yes. Distillation is another way to improve efficiency. You have a large, accurate teacher model, and you train a smaller student model to copy its behavior. The student model is often much faster and more efficient, and it can still achieve good accuracy. There are a few different distillation techniques, like data distillation, knowledge distillation, and on-policy distillation. OK. Those are the methods that might change the output a little bit. What about the output preserving methods? I've heard of flash attention. Flash attention is really cool. It's specifically designed to optimize the self-attention calculations within the transformer. It basically minimizes the amount of data movement needed during those calculations, which can be a big bottleneck. The great thing about flash attention is that it doesn't change the results of the attention computation just the way it's done, so the output is exactly the same. And prefix caching. That seems like a good trick for conversational applications. Yeah. Prefix caching is all about saving time when you have repeating parts of the input, like in a conversation where each turn builds on the previous ones. You cache the results of the attention calculations for the initial part of the input, so you don't have to redo them for every turn. Google AI Studio and Vertex AI, they both have features that use this idea. So it's like remembering what you've already calculated so you don't have to do it again. What about speculative decoding? Speculative decoding is pretty clever. You use a smaller, faster, drafter model to predict a bunch of future tokens, and then the main model checks those predictions in parallel. If the drafter is right, you can accept those tokens and skip the calculations for them, which speeds up the decoding process. The key is to have a drafter model that's well aligned with the main model, so its predictions are usually correct. And then there's the more general optimization techniques, like batching and parallelization. Right. Batching is where you process multiple requests at the same time, which can be more efficient than doing them one by one. Parallelization is about splitting up the computation across multiple processors or devices. There are different types of parallelization, each with its own trade-offs. So there's a whole toolbox of techniques for making these models run faster and more efficiently. Now, before we wrap up, I'd love to hear some examples of how all this is being used in practice. Oh, the applications are just exploding. It's hard to even keep track. In code and math, LLMs are being used for code generation, completion, refactoring, debugging, translating code between languages, writing documentation, and even helping to understand large code bases. We have models like AlphaCode2 that are doing incredibly well in programming competitions. And projects like FunSearch and AlphaGeometry are actually helping mathematicians make new discoveries. In machine translation, LLMs are leading to more fluent, accurate, and natural sounding translations. Text summarization is getting much better, able to condense large amounts of text down to the key points. Question answering systems are becoming more knowledgeable and precise, thanks in part to techniques like ARAG. Chatbots are becoming more human-like in their conversations, able to engage in more dynamic and interesting dialogue. Content creation is also being transformed, with LLMs being used for writing ads, scripts, and all sorts of creative text formats. And we're seeing advancements in natural language inference, which is used for things like sentiment analysis, analyzing legal documents, and even assisting with medical diagnoses. Text classification is getting more accurate, which is useful for spam detection, news categorization, and understanding customer feedback. And LLMs are even being used to evaluate other LLMs, acting as those moderators we talked about. In text analysis, LLMs are helping to extract insights and identify trends from huge data sets. It's really an incredible range of applications. And we're only scratching the surface, right? Especially with the multimodal capabilities coming online. Exactly. Multimodal LLMs, they're enabling entirely new categories of applications, where you combine text, images, audio, and video. We're seeing them being used in creative content creation, education, assistive technologies, business, scientific research, you name it. It's truly a transformative technology. Well, I have to say, this has been a fascinating deep dive. We started with the basic building blocks of the transformer architecture, explored the evolution of all these different LLM models, got into the nitty gritty of fine tuning and evaluation, and even learned about the techniques used to make them faster and more efficient. It's incredible to see how far this field has come in such a short time. Yeah, the progress has been remarkable. And it seems like things are only accelerating. Who knows what amazing things we'll see in the next few years? That's a good question. And it's one I think our listeners should ponder as well. Given the rapid pace of innovation, what new applications do you think will be possible with the next generation of LLMs? What challenges do you think we need to overcome to make those applications a reality? Let us know your thoughts, and thanks for joining us for another deep dive. Thanks everyone, it's been a pleasure.\n",
            "\n",
            "Metin 'transcription.txt' dosyasına kaydedildi.\n"
          ]
        }
      ]
    }
  ]
}